# Local Interpretable Model-agnostic Explanations (LIME)

Local surrogate models are models that enable model interpretability and are used to explain individual predictions of black box machine learning models. LIME is a technique that does not try to explain the whole model, instead LIME tries to understand the model by perturbing the input of data samples and comprehending how the predictions change. A single data sample is modified adjusting some feature values and the resultant output impact is observed. This is often linked to what human interests are when the output of a model is observed.


The goal is to understand the decision making process of the black box model i.e. why and how the machine learning model made a certain prediction. The idea of LIME is quite intuitive. Imagine you only have the black box model where you can input data points and get the predictions of the model. LIME tests what happens to the predictions when you give variations of your data into the machine learning model. LIME generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation.
