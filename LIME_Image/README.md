# LIME on IMAGE
LIME for images works differently than LIME for tabular data and text. Intuitively, it would not make much sense to perturb individual pixels, since many more than one pixel contribute to one class. Randomly changing individual pixels would probably not change the predictions by much. Therefore, variations of the images are created by segmenting the image into "superpixels" and turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be turned off by replacing each pixel with a user-defined color such as gray. The user can also specify a probability for turning off a superpixel in each permutation.

# A computer vision example
The figure illustrates how LIME is used for classification of images. Let us suppose we have to explain a classifier which predicts the likelihood of the image containing an Egyptian cat. The image part b  is taken and split into easily interpretable components. As seen in the part c, a dataset of perturbed instances is generated by turning off (turning them grey here) some of the interpretable components. For every instance of perturbation, we are served with the probability of an Egyptian cat being in the image as per the model. We then try to understand a locally weighted simple (linear) model on such a dataset. The emphasis is more on erring in perturbed instances which best match the original image. Ultimately, we provide the super-pixels showing the highest positive weights as our explanation (part d). We distinguish it by turning everything else grey.

<img width="922" alt="lime-computervision" src="https://user-images.githubusercontent.com/72971618/108672007-5a970680-7507-11eb-9038-4f55e2760160.png">